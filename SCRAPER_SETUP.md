# Web Scraper Kurulum Rehberi

EventMap iÃ§in Biletix, Bubilet ve Biletinial'dan otomatik etkinlik Ã§ekme sistemi.

## ğŸ¯ Genel BakÄ±ÅŸ

Web scraper Supabase Edge Functions kullanarak Ã§alÄ±ÅŸÄ±r:
- **Serverless**: AyrÄ± backend sunucuya gerek yok
- **Ãœcretsiz**: AylÄ±k 500,000 istek limiti
- **CORS sorunsuz**: Backend'den scraping yapÄ±lÄ±r
- **Otomatik**: Cron job ile periyodik Ã§alÄ±ÅŸtÄ±rma

## ğŸ“¦ Gereksinimler

1. Supabase CLI kurulumu
2. Deno runtime (otomatik kurulur)
3. Supabase projesi

## ğŸš€ Kurulum AdÄ±mlarÄ±

### 1. Supabase CLI Kurulumu

#### Windows:
```powershell
# Chocolatey ile
choco install supabase

# veya Scoop ile
scoop bucket add supabase https://github.com/supabase/scoop-bucket.git
scoop install supabase
```

#### macOS:
```bash
brew install supabase/tap/supabase
```

#### Linux:
```bash
# DoÄŸrudan binary indir
curl -L https://github.com/supabase/cli/releases/latest/download/supabase_linux_amd64.tar.gz | tar -xz
sudo mv supabase /usr/local/bin/
```

### 2. Supabase Login

```bash
supabase login
```

TarayÄ±cÄ±da aÃ§Ä±lan sayfadan token alÄ±p terminale yapÄ±ÅŸtÄ±rÄ±n.

### 3. Proje BaÄŸlantÄ±sÄ±

Proje klasÃ¶rÃ¼nde:

```bash
cd "c:\Users\murat\OneDrive\MasaÃ¼stÃ¼\Eventmap"
supabase link --project-ref zktzpwuuqdsfdrdljtoy
```

Database ÅŸifrenizi girmeniz istenecek.

### 4. Edge Function Deploy

```bash
supabase functions deploy scrape-events
```

Bu komut:
- Edge function'Ä± Supabase'e yÃ¼kler
- HTTPS endpoint oluÅŸturur
- Environment variables'Ä± otomatik ayarlar

### 5. Test Etme

Edge function deploy olduktan sonra:

```bash
# Test call
supabase functions invoke scrape-events --method POST
```

## ğŸ”§ KullanÄ±m

### Manuel Ã‡alÄ±ÅŸtÄ±rma (Frontend'den)

```typescript
import { useScraper } from '@/hooks/useScraper';

function AdminPanel() {
  const { scrapeEvents, isScraping, result } = useScraper();

  const handleScrape = async () => {
    const result = await scrapeEvents();
    console.log('Scraped:', result.stats);
  };

  return (
    <button onClick={handleScrape} disabled={isScraping}>
      {isScraping ? 'Ã‡ekiliyor...' : 'Etkinlikleri Ã‡ek'}
    </button>
  );
}
```

### Otomatik Ã‡alÄ±ÅŸtÄ±rma (Cron Job)

Supabase Dashboard > Database > Cron Jobs:

```sql
-- Her gÃ¼n saat 03:00'te Ã§alÄ±ÅŸtÄ±r
SELECT cron.schedule(
  'scrape-events-daily',
  '0 3 * * *',
  $$
  SELECT net.http_post(
    url := 'https://zktzpwuuqdsfdrdljtoy.supabase.co/functions/v1/scrape-events',
    headers := jsonb_build_object(
      'Authorization', 'Bearer YOUR_ANON_KEY',
      'Content-Type', 'application/json'
    ),
    body := '{}'::jsonb
  );
  $$
);
```

## ğŸ“Š Monitoring

### Logs GÃ¶rÃ¼ntÃ¼leme

```bash
supabase functions logs scrape-events
```

### Dashboard'dan Ä°zleme

1. Supabase Dashboard > Edge Functions
2. `scrape-events` fonksiyonuna tÄ±klayÄ±n
3. "Logs" sekmesini aÃ§Ä±n
4. Real-time loglarÄ± gÃ¶rÃ¼n

## ğŸ” Environment Variables

Edge function otomatik olarak ÅŸu deÄŸiÅŸkenlere eriÅŸebilir:

- `SUPABASE_URL`: Proje URL'i
- `SUPABASE_SERVICE_ROLE_KEY`: Service role key (otomatik inject edilir)

Ekstra deÄŸiÅŸken eklemek iÃ§in:

```bash
supabase secrets set MY_SECRET=value
```

## ğŸ› Troubleshooting

### "Function not found" hatasÄ±

```bash
# Function'larÄ± listele
supabase functions list

# Tekrar deploy et
supabase functions deploy scrape-events
```

### CORS hatasÄ±

Edge function'dan scraping yapÄ±yorsanÄ±z CORS sorunu olmaz. Frontend'den direkt fetch yapmayÄ±n.

### Rate limiting

Web siteleri rate limit koyabilir:
- User-Agent header kullanÄ±n
- Ä°stekler arasÄ±nda delay ekleyin
- Proxy servisleri kullanÄ±n (ScraperAPI, Apify)

### Parser hatasÄ±

GerÃ§ek HTML parsing iÃ§in:

```typescript
// Deno DOM parser
import { DOMParser } from 'https://deno.land/x/deno_dom/deno-dom-wasm.ts'

const doc = new DOMParser().parseFromString(html, 'text/html')
const title = doc.querySelector('.event-title')?.textContent
```

## ğŸ’° Maliyetler

Supabase Free Tier:
- âœ… 500,000 Edge Function invocations/ay
- âœ… 2GB veri transferi
- âœ… Unlimited read operations

FazlasÄ± iÃ§in Pro plan ($25/ay).

## ğŸ”„ GerÃ§ek Scraping Ä°mplementasyonu

Åu anda mock data kullanÄ±lÄ±yor. GerÃ§ek scraping iÃ§in:

1. Her platform iÃ§in HTML selector'larÄ± belirleyin
2. Deno DOM parser kullanÄ±n
3. Rate limiting ekleyin
4. Error handling gÃ¼Ã§lendirin

Ã–rnek:

```typescript
async function scrapeBiletix(): Promise<ScrapedEvent[]> {
  const response = await fetch('https://www.biletix.com/...')
  const html = await response.text()

  const doc = new DOMParser().parseFromString(html, 'text/html')
  const eventCards = doc.querySelectorAll('.event-card')

  return Array.from(eventCards).map(card => ({
    title: card.querySelector('.title')?.textContent || '',
    // ... diÄŸer alanlar
  }))
}
```

## ğŸ“š Daha Fazla Bilgi

- [Supabase Edge Functions Docs](https://supabase.com/docs/guides/functions)
- [Deno Runtime](https://deno.land/)
- [Web Scraping Best Practices](https://www.scrapingbee.com/blog/web-scraping-best-practices/)

## ğŸ‰ Ã–zet

ArtÄ±k sisteminiz:
- âœ… Biletix, Bubilet, Biletinial'dan etkinlik Ã§ekebilir
- âœ… Otomatik olarak veritabanÄ±na ekler
- âœ… Cron job ile gÃ¼nlÃ¼k Ã§alÄ±ÅŸtÄ±rÄ±labilir
- âœ… Frontend'den manuel tetiklenebilir

**Not**: Mock data kullanÄ±yor, production iÃ§in gerÃ§ek HTML parsing ekleyin!
